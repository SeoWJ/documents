# RedHat Openstack 교육

- 김주효 강사 / juhyokim@nobreak.co.kr
- RedHat Openstack 16 version
- RHCSA 자격증이 있어야 오픈스택 자격증 취득 가능.

## Chapter 0. 환경

### 인프라 환경 구성요소
- P9 장표 참조
- bastion : 외부와 연결하는 관리용 시스템. 라우터 역할도 수행. 실습에서는 사실상 라우터라고 봐도 좋다.
- workstation : 오픈스택과는 상관없는 시스템. 평소 서버/시스템 관리하는 시스템이라고 봐도 됨. 평소 사용하는 노트북 같은거라 봐도 좋다. 보통 CLI 이기 때문에 GUI쓰고 싶을 때 이걸 쓰면 된다.
- director
    - 레드햇 오픈스택은 플랫환경이 아니라 계층구조로 구성된다.
    - undercloud : 디렉터 + 네트워크구성까지 언더클라우드라 본다.
    - 계층적으로 구성하는 이유는 오버클라우드라는 환경은 사용자에게 서비스를 제공하기 위한 환경. 이 서비스들을 하나하나 직접설치하기 귀찮으니까 디렉터를 통해 일괄적으로 배포/관리가 가능.
    - 플랫환경 : 디렉터 없이 오버클라우드 구성만 있는 환경.
- power : 계층구성을 할 때 필수. 이름 그대로 파워관리를 해줌. IPMI. 노드들을 원격으로 전원관리 가능하게 만들어 둔 시스템.
- utility : 도구 제공 시스템. 실습환경은 유틸리티 시스템에서 레드햇 idm이란 서비스 사용. 일반적으로는 IPA, LDAP라고 부름(외부 디렉토리 서비스). 오버클라우드에서 사용하는 DNS서비스도 여기에 정의된다.
- controller : 이름 그대로 오픈스택환경에서 클라우드 시스템을 관리하는 관리노드. 한대로 그려져있는데 실제 운영환경에서는 클러스터 구성이 필요하다. 저 서버하나 망가지면 박살난다.
- compute : 우리가 실행할 인스턴스.
- ceph0 : 스토리지 구성. 오픈스택 구성할 때 저장공간(스토리지)를 따로 구성하지 않아도 되긴하나, 실제 환경에서는 당연히 따로 구성한다. object스토리지. 3대 이상의 클러스터 구성(한대로만 구성하면 의미 없음). 
* blob : 블록단위로 스토리지를 저장하는 파티션이나 볼륨단위를 만들어 사용하는 방식. 가상머신의 데이터들을 파일 하나로 만들어 저장. 이걸 블롭스토리지라 한다.
* object storage : 나는 파일을 하나 저장하는데, 이를 여러 데이터 단위로 쪼개서 데이터 하나를 저장하는데도 단순 저장이 아니라 복제를 해서 여러 스토리지에 같은 데이터를 저장.

- 네트워크는 3종류. 익스터널, 매니지먼트, 프로비저닝
- 익스터널과 인터널을 라우터를 통해 연결할 것. 그 시스템과 연결된 네트워크를 External이라 한다.
- Management 네트워크
    - 스토리지 관리용 / api관리용 등으로 나뉨. VLAN으로 쪼개서 관리.
    - 오버클라우드끼리만 연결되어 있다. 관리는 컨트롤러를 통해 나머지를 관리하기에 같이 연결될 필요가 없다.
- Provisioning 네트워크
    - 디렉터 네트워크가 연결되어 있다.

### 네트워크 구성
- P11 장표
- 인터널, 테넌트(프로젝트) : 컴퓨트에만 연결되어 있음.
- 스토리지 매니지먼트 : 컴퓨트와 스토리지에만 연결되어 있음.
- 매니지먼트 : 전부다

### 오픈스택 부팅
- director 환경에서
    - `source ~/overcloudrc` : 오버클라우드 환경으로 변경
    - `source stackrc` : 언더클라우드 환경으로 변경
    - `openstack compute service list`  : 활성화 된 서비스 리스트 출력
    - `openstack baremetal node list` : 컨트롤러 컴퓨트 셰프 시스템에 대해 부팅상태 확인 가능
    - `openstack baremetal node power on <이름>` : 부팅을 해주는 명령어. 부팅할때 IPMI를 통해서 부팅 명령어 전달
    - `ssh controller0` : 컨트롤러 접속
    - `sudo pcs cluster start --all` : PCS명령어로 클러스터를 활성화 하겠다. 컨트롤러 최초 부팅후에는 클러스터부터 활성화해야함
    - `sudo podman ps` : 실행된 컨테이너 목록 확인. ceph-mon-controller0에서 작업 해야 한다.
    - `sudo -i`로 루트 변환후 `podman exec chep-mon-controller0 ceph osd unset noout` : 컨테이너 내부에 뒤의 명령어 전달
    - `ceph ost unset XXX` : 셰프 클러스터 명령어들 플래그들 비활성화. 오브젝트 스토리지 기반으로 백업등의 기능이 동작하게 되어있다. 정상 종료시에는 저걸 다 비활성화 해야함.
        - noout
        - norecover
        - norebalance
        - nobackfill
        - nodown
        - pause
    - exit두번 진행하여 루트에서 빠져나가고 디렉터로 다시 돌아와서 undercloud환경에서 `openstack baremetal node power on compute0/compute1` : vm부팅.
    - 마지막으로 키오스크로 나가서 `rht-vmctl start all` : 아침에 오면 매일 이걸 해줘야 한다.

    - 순서대로 정리하자면,
        - director power
        - director에서 ceph 부팅
        - director에서 controller 부팅
        - 이 두가지 시스템은 부팅 순서 무관. 둘다 부팅된 이후 클러스터 활성화
        - ceph스토리지에 대한 플래그 설정
        - director에서 컴퓨트 실행
    - 종료할때는 역순. 플래그 설정할 때는 unset을 set으로 적용하면 된다.

    - `rht-vmctl XXX`
        - start : 부팅할 때 사용. 아침에 부팅할 때
        - reset : 잘못설정하거나 할 때 리셋. 현재상태로 되돌아간다. all을 추가로 입력하면 전체선택. undercloud, overcloud등으로 그룹적용도 가능하다.

### lab. 실습/연습 환경 제공
- ssh student@workstation에 접속하여 입력한다.
- `lab XXX start`
    - start : 시작
    - grade : 채점할때.
    - finish : 실습한 내용 제거


## Chapter 1. Red Hat Openstack Platform 소개

### 가상 사용자
- P3 장표 참조.
- 가상 사용자 : 인프라 환경을 접하는 우리를 의미. 엔지니어, 운영자, 개발자, 일반 사용자 모두를 포함.
- 우리가 진행하는 이 과정의 주요 대상은 Domain Operator 기준으로 많이 설명되어 있다.
- 제일 가운데, 로우레벨에 있는 사람들이 데이터센터를 관리하는 사람. 그 다음으로 물리적인 인프라를 관리하는 사람. 우리는 이 영역은 다루지 않는다.
- Domain Operator : 구성되어있는 오픈스택 환경을 운영하는 사람들.
    - 프로젝트(작업 단위. 내가 사용하려는 리소스들의 그룹을 의미. 옛날 용어로 Tenent) 생성 및 관리.
    - 사용자 및 사용자 역할 할당.
    - 도메인에서 리소스와 기타 작업 관리.
        - 도메인이 상위개념. 도메인이 클라우드 환경이라고 봐도 무방하다.
        - 고객마다 각각의 도메인을 분리하여 제공한다.
    - 프로젝트 - 리소스 및 배포된 애플리케이션 격리
    - 하나 이상의 오픈스택 ID 서비스 도메인에 속함
    - 도메인 - 사용자, 프로젝트, 리소스 포함
    - 여러 도메인 지원
    - 직접 설치하지는 않지만 요구사항 이해 필요
    - 클라우드 사용자 지원을 위한 경험 및 기본지식 필요

- 도메인 운영자가 지원하는 다른 가상 사용자
    - 애플리케이션 개발자
        - 일반 사용자라 봐도 무방하다.
        - 코드 개발자, 유지관리자, 기타 클라우드 사용자
    - 프로젝트 소유자
        - 도메인 운영자로부터 관리 권한 위임
        - 프로젝트 별 사용자 및 역할 할당 관리

- 가상 사용자 별 설명
    - 프로젝트 소유자 : 애플리케이션의 개발 또는 유지관리를 관리할 책임이 있음.
    - 인프라 아키텍트 : 분배된 Overcloud 배포 용량 및 구성 설계
    - 도메인 관리자 : 리소스 할당, 사용자 액세스 및 배포 지원 관리
    - 서비스 관리자 : 오픈스택 인프라 리소스 구성 요소를 구현, 지원 및 확장.

### 인스턴스 시작
- 대부분의 명령어는 `openstack`으로 시작
- 일반적으로 `openstack <관리대상> <동작>`의 순서로 되어있음.
- 인증을 마친 사용자에게 접근 권한을 부여하며, 인증에 필요한 값은 환경변수 or 매개변수로 제공(환경 변수 권장)
- `cat <유저명>-finance-rc` 명령어로 사용자 정보를 확인할 수 있다.
    - OS_USERNAME : 사용자 이름
    - OS_PASSWORD : 패스워드
    - OS_PROJECT_DOMAIN_NAME : 도메인이란 프로젝트와 사용자에 대한 상위 개념. 도메인을 여러개 사용하는 경우 지금의 프로젝트가 어느 도메인에 있는지에 대한 정보.
    - OS_PROJECT_NAME : 프로젝트의 이름.
    - OS_IMAGE_API_VERSION
    - OS_IDENTITY_API_VERSION : 대부분 요즘은 3버전 사용. 2일 경우 도메인이라는 개념을 사용하지 않는다는 문제점이 있다. 또한 오픈스택도 리눅스와 마찬가지로 사용자와 그룹이 있는데 2에는 그룹이 없다.
    - OS_AUTH_URL : 인증 서버의 주소. 사용자 이름, 패스워드 등을 어디를 통해 인증하겠다는 정보. 일반적으로 컨트롤러 노드에서 동작하므로 컨트롤러 노드의 ip주소와 동일.
    
- `source admin-rc` or `source <유저명>-finance-rc` 명령어로 접속하여 사용한다. 이 과정이 인증이며, 이 과정 없이 명령어를 사용하면 실행되지 않는다.
- `openstack`이라고 입력하면 명령어에 대한 리스트를 확인할 수 있다.

- 명령어 리스트
    - `openstack project list` : 프로젝트의 리스트를 출력해준다.
        - admin : 관리용 프로젝트
        - service : 오픈스택 내부에서 동작하는 각 서비스들이 동작하는데 있어 필요한 프로젝트. 있는지만 보면 됨. 쓰지는 않는다.
        - 그 외 : 필요할 때마다 그때그때 만들어서 사용.
    - `openstack project list --help` : 출력방식이 맘에 안들 때 줄수있는 인자 리스트에 대한 도움말들을 출력
    - `openstack network list` : id, name, subnet을 출력.
    
    - 출력형식 변경
        - `--fit-width` : 가로 길이를 맞춰준다.
        - `--domain` : 도메인만 출력
        - `-f` : 출력 방식 결정. csv, json, yaml 등을 설정할 수 있다. 기본적으로는 테이블로 출력.
        - `--max-width` : 행 넓이를 제한할 수 있다.
        - `-c` : 컬럼인 경우 아이디만 보겠다. 각 컬럼별로 가지고있는 아이디들은 알고있어야 쓰겠지?

- 프로젝트를 사용하여 애플리케이션 및 네트워크 관리
    - admin 권한을 가진 사용자만 가능
    - 어떤 사용자에 대해 어떤 역할을 할당한다의 방식으로 수행하게 될 것.
    - member : 관리자는 아니고 일반적인 사용자를 의미한다.
    - 많은 부분은 멤버로도 작업이 가능하다. 일부 부분의 경우에만 어드민 권한이 필요하다.

- 인스턴스 : 클라우드에서 사용하는 가상머신을 지칭하는 용어.
    - 인스턴스는 사용하는 데이터들을 임시 스토리지를 이용하여 저장하도록 되어있다. 아예 휘발성 데이터란 의미는 아니고 인스턴스를 유지하는 동안은 유지되는 공간이다. 인스턴스 삭제시 데이터도 같이 삭제된다.
    - 영구 스토리지 : 필요에 따라 연결할 수 있는데, 인스턴스를 지워도 데이터를 남겨둘 수 있다.
    - 이미지 : 인스턴스를 만들 때 필요한 개체들 중 하나로, 가상머신이 동작할 수 있게 해주는 디스크 파일이다. 포맷은 RAW or QCOW2 사용. 이미지를 업로드 및 저장은 멤버도 가능하나 공용으로 사용하는 이미지는 어드민만 가능하다.
    - 플레이버 : 인스턴스의 계산, 메모리 및 스토리지 용량을 지정. 어드민만 플레이버를 생성할 수 있다. 멤버는 선택만 가능. 
    - `openstack server create --image rhel8 --flaver small --network finance-network1 --wait finance-server1` 인스턴스 생성 명령어 예시.
        - wait 옵션의 경우 내가 명령어를 실행하면 실행되고 나서 실제 인스턴스가 만들어질때까지 기다리겠다는 의미. 이걸 설정 안하면 만들어졌을때 프롬프트가 생성되고 나서 인스턴스가 잘 실행되고 났을 때 응답이 와서 새 프롬프트가 뜬다. 급한게 아니라면 쓰는게 좋다. 만들어졌다고 생각했는데 오류가 발생했을 수도 있으므로.

- 인스턴스 콘솔에 액세스
    - 당연히 콘솔을 쓰려면 GUI환경이어야 한다.
    - `openstack console url show` 명령으로 주소 확인
    - `openstack console log show <SERVER_ID>` : 로그를 띄워준다. 부팅과정에서 어떤걸 설정했는지 등을 볼 수 있다.

- Openstack 대시보드에서 리소스 관리
    - 브라우저를 이용하여, http://dashboard.overcloud.example.com 강의실의 경우는 여기서 확인할 수 있다.
    - 사용자는 Domain : default / ID : admin / PW : redhat으로 접속 가능
    - compute - instances - launch instance 버튼으로 인스턴스 생성 가능.

### Openstack 플랫폼 서비스 설명
- Cinder
    - 블록 스토리지 서비스.
    - 우리가 사용하는 인스턴스들이 추가적인 저장공간이 필요하다 할 때 연결해 주는 서비스.

- Glance
    - 이미지 서비스.
    - 우리가 사용할 이미지 파일들을 관리.
    - 이미지 : 내가 사용할 인스턴스가 어떤 운영체제를 쓸 것인지, 어떤 프로그램을 쓸 것인지에 관한 것.

- Heat
    - 오케스트레이션 서비스
    - 편의를 위해 자주 사용하는 서비스
    - 템플릿으로 여러 리소스들을 일괄관리하는 용도

- Horizon
    - 대시보드 서비스
    - 오픈스택 관리 및 인스턴스 시작을 위한 그래픽 인터페이스 제공

- Ironic
    - 베어메탈 프로비저닝 서비스.
    - 물리 하드웨어를 프로비저닝 (PXE, IPMI 드라이버 제공)

- Keystone
    - ID 관리 서비스
    - 가장 중요한 서비스라 볼 수 있다.
    - 인증 및 권한 부여를 담당하는 서비스
    - 도메인으로 경계 구분

- Neutron
    - 네트워킹 서비스
    - SDN으로 인스턴스/라우터/네트워크 등 연결 구성
    - 시큐리티 그룹 설정
    - floating ip 설정

- Nova
    - 컴퓨트 서비스
    - 우리가 사용할 인스턴스에 대한 관리를 제공.
    - 인스턴스를 어떤 노드에서 실행하는지 선택. 선택했으면 배치하고 동작하고 실행할 수 있도록 해주는 부분. 이후 관리까지 모두 담당.
    - 하이퍼바이저에 libirth, qemu, kvm 사용.

- Swift
    - 오브젝트 저장소 서비스.
    - 오브젝트 스토리지 관리.
    - 수평적 확장과 이중화 제공을 위해 분산 구조 사용.
    - 이미지 서비스의 스토리지 백엔드로 사용 가능.
    - 레드햇 오픈스택은 Ceph로 대체되어 사용.

- Placement
    - 배치 서비스.
    - 클라우드 리소스 인벤토리 및 사용을 추적해 리소스의 효율적인 관리 지원.

- Manila
    - 공유 파일 시스템 서비스.

- ※ 스토리지 종류별 설명
    - 인스턴스를 만들어 사용하는데 있어서 인스턴스에 저장공간을 만드는 것은 블록 스토리지.
    - 인스턴스와 상관없이 내가 개인적으로 저장하거나 오픈스택을 하면서 따로 사용하는데 쓸거는 오브젝트 스토리지
    - 블록스토리지는 한번에 하나밖에 연결이 안됨. 한 블록을 인스턴스 여러개에 동시에 연결하는건 기본적으로 안된다. 이걸 가능하게 하는 서비스가 마닐라.

- Telemetry Service
    - Ceilometer라는 이름의 서비스를 사용.
    - Ceilometer는
        - Aodh
        - gnochi
        - panko
        - 이상의 3가지 서비스로 세분화
    - 오픈스택환경에서 어떤 인스턴스가 얼마나의 리소스를 쓰는지 사용량 측정 및 과금 정책 관여

- Kolla
    - 컨테이너 배포 서비스.
    - Ansible로 자동화.

- Octavia
    - 부하분산 서비스
    - HAProxy 사용.

- TripleO
    - 배포 서비스.
    - 배포, 운영 모니터링, 업그레이드 기능까지 제공.

- Openstack 서비스의 상태 나열
    - 컨트롤러 노드에 접속하여 사용 가능.
    - `podman ps` 명령어로 실행중인 컨테이너를 나열할 수 있다.
    - `podman ps -a` : 중지된 서비스까지 확인 가능.
    - `podman logs` : 해당 컨테이너의 로그 확인 가능.
    - `podman top` : 해당 컨테이너 내부에서 돌아가는 프로세스의 정보 확인.

### 단일 사이트 Overcloud 배포
- P33 장표 참조.
- 하나의 사이트에 대한 구조
- HyperConverged : 컴퓨트 + 스토리지의 결합 형태.
- 외부용, 관리용, 프로비저닝용 으로 오버클라우드의 네트워크는 3개씩은 필요하다.
- 3대이상의 클러스터형 컨트롤러 노드를 사용하여 단일노드 장애를 방지하여 고가용성을 제공해야 한다.



## Chapter 4. 비공용 인스턴스를 시작하도록 리소스 구성


### 이미지와 플레이버를 지정하여 가상 시스템 정의

- 이미지 소개
    - 인스턴스 실행 시 필수. 
    - 부팅이 가능한 운영체제가 설치되어 있는 파일.
    - 배포시 애플리케이션 사용 사례에 따라 특정 플레이버 구성 요구.
    - 이미지 서비스(Glance)에서 관리
    - 레드햇에서는 cloud-init으로 사전 구성된 게스트 이미지를 제공.
    - 모든 사용자가 이미지 업로드 가능.
    - Ceph 미사용시 glance 명령어로 이미지 형식을 레드햇 지원 형식으로 변환 가능.
    - Ceph 사용시 자동 형변환 지원(RAW 파일로 전환)
    - 이미지 사용시 저장소를 필수로 저장해야 한다. 기본은 로컬 스토리지를 사용한다.

- 이미지 스토리지 설명
    - 이미지에는 부팅 가능 OS를 포함하여 필요한 최소한의 소프트웨어가 포함.
        - 도커의 이미지는 운영체제 없이 어플리케이션만 저장되어 있는 점 참고.
    - Ceph images 풀에 저장 및 이미지 서비스에서 제공
    - 기본 순서
        - 인스턴스 시작 시 컴퓨트 서비스가 이미지 서비스 요청
        - 해당 컴퓨트 노드의 libvirt 캐시에 이미지 복사(이후 동일 이미지 요청 시 사용)
        - 가상 디스크 생성 및 플레이버에 맞게 크기 조정.(이미지를 플레이버에 맞게 조정한다는 의미)
    - Ceph 사용 시
        - images 풀에서 복제본을 생성(플레이버 사이즈)
        - vms 풀에 인스턴스 블록 장치 생성

- 이미지 속성
    - 각 이미지에는 이미지와 연결된 메타데이터 존재.
    - Visibility(접근제어)
        - private : 소유자만 접근 가능
        - shared : 기본값으로 동일 프로젝트 사용자들 접근 가능
        - public : 모든 사용자 접근 가능
        - community : 모든 사용자 접근 가능
    - Protected(삭제 방지) : 아예 삭제불가는 아니고, 프로텍트 해제 후 삭제하도록 유도. 안전장치라 생각하면 된다.
    - os_name(운영체제 종류 지정)
    -libosinfo 패키지를 설치하여 `osinfo-query os` 명령어를 통해 이미지의 운영체제를 확인할 수 있다.

- commands
    - `sudo podman exec ceph-mon-controller0 ceph osd lspools` : 컨트롤러 노드에서 실행. 풀 목록 확인
    - `sudo podman exec ceph-mon-controller0 rbd ls --pool <pool name>` : 지정된 풀에 저장된 데이터 확인
    
    - `openstack image create --min-disk <size> --min-ram <size> --file <file path> <image name>` : 이미지 생성 명령어. 디스크 및 램의 최소 사양을 설정해줄 수 있다.
    - `openstack image set --min-disk <size> --min-ram <size> <image name>` : 만들어둔 이미지의 속성 변경

- 플레이버 소개
    - 하드웨어 리소스 템플릿. 인스턴스를 만들때 있어서 사용할 리소스를 어떻게 설정할 것인지. 인스턴스에 대한 할당량.
    - 디스크와 메모리 크기 및 코어 수 포함
    - 추가 임시스토리지 또는 스왑디스크 및 각종 메타데이터 지정 가능.
    - 관리자만 생성 가능

- 플레이버 관리
    - 컴퓨트 노드의 리소스 확인 후 배포 예약 진행(해당 노드 없을 시 실패)
    - 디폴트 도메인의 어드민 프로젝트에 대한 어드민 역할이 할당된 사용자만 관리 가능.
    - private으로 생성 시 특정 프로젝트에서만 사용 제한.

- 플레이버에 스토리지 정의
    - 인스턴스에 사용할 수 있는 가상 스토리지 정의
    - 루트디스크, 임시디스크, 스왑디스크를 비롯한 다양한 스토리지 리소스 존재
        - 루트 디스크 : 운영체제가 포함된 이미지를 템플릿으로 사용하여 생성
        - 임시 디스크 : 인스턴스에 연결된 새로운 비정형 가상 장치
        - 스왑 디스크 : 전체 장치가 운영체제 실행을 위한 스왑 공간으로 활성화
    - 임시장치는 vms라는 Ceph 풀에 생성 및 저장(인스턴스 삭제 시 함께 삭제)

- 플레이버의 기타 매개변수
    - 플레이버 사용자 지정은 extra_specs 요소에서 구현(자유양식의 특성 정의)
    - 컴퓨트 노드의 해당 key-value 설정과 일치하게 설정
    - 키 목록
        - hw:action
        - hw:NUMA_def
        - hw:watchdog_action
        - hw_rng:action
        - quota:option
        - P134 참조


### 프로젝트 네트워크 및 서브네트워크 생성

- 오픈스택 테넌트 네트워크 설명
    - 클라우드 사용자가 사용하는 가장 일반적인 네트워크 유형
    - 테넌트 네트워크는 격리형으로 설계
    - 독립적인 DHCP 제공 및 네트워크 격리로 IP주소 범위 중첩 허용

- commands
    - `openstack network create <network name>` : 네트워크 생성
        - `--share` / `--no-share` : 서로다른 네트워크에서 공유 / 프로젝트 내에서만 사용. 기본값은 노 쉐어
        - `--project <pj name>` : 프로젝트에 할당
        - `--external` / `--internal` : 외부/내부. 기본값은 인터널.
    - `openstack subnet create <subnet name>` : 서브넷 생성
        - `--network <nt name>` : 어떤 네트워크에 대한 ip설정인지 설정.
        - `--dhcp` / `--no-dhcp`
        - `--subnet-range <subnet range>` : 네트워크 대역 설정
        - `--allocation-pool` : 지정된 네트워크 대역 내에서 어디부터 어디까지 쓸 것인지 설정

### 비공용 인스턴스 시작 및 확인

- 인스턴스 시작
    - 인스턴스 시작시 최소한의 리소스는 플레이버, 이미지, 네트워크.
    - 프로젝트에 하나의 네트워크만 정의되어 있는 경우 해당 네트워크 자동 선택
    - `openstack XXX list` / `openstack XXX show` 명령어로 가용 리소스 확인

- 실행중인 인스턴스 확인
    - 하나의 NIC가 있으면 동일한 프로젝트 내에서 접근 가능.
    - 여러개의 NIC 구성 시 다양한 프로젝트 네트워크에서 접근 가능.
    - 명령어 / 대시보드를 통한 상태 확인 및 VNC 콘솔을 이용한 접속으로 확인.
        - `openstack console show`
        - `openstack console url show`

- commands
    - `openstack server create <server name>`
    - `openstack server delete <server name/id>`



## Chapter 5. 가상 시스템을 위한 시스템 디스크 구성

### 임시 디스크 구성

- 레드햇 오픈스택 플랫폼의 스토리지
    - ceph를 블록 스토리지 서비스의 백엔드로 사용.
    - 기존 엔터프라이즈급 스토리지 시스템과의 통합도 지원(DAS/NAS/SAN)
        - DAS : Direct Access. 케이블로 직접 연결한 스토리지
        - NAS : 네트워크로 연결한 스토리지. NFS처럼 서버쪽에서 파일 시스템까지 다 만들어서 디렉토리/파일 형태로 제공
        - SAN : 서버쪽에서는 디스크 자체를 제공을 하고, 우리가 직접 파일 시스템을 만들어 마운트해서 사용하는 형식.(ex. ISCSI)
    - 클라우드 기반 인스턴스에서는 가상 시스크를 직접 연결
    - 로컬 스토리지를 확장하는 방법으로 외부 공유 스토리지를 제공.
    - 블록 스토리지 서비스 없이는 모든 인스턴스 디스크는 임시적임.(인스턴스 종료시 삭제). 즉 기본 서비스는 블록 스토리지.
    - 임시 스토리지는 배포된 인스턴스에서 사용된 블록 디스크 장치 및 스왑 장치 모두.
    - 인스턴스의 스토리지를 확장하려면 추가 가상 디스크를 프로비저닝.
    - 영구 스토리지 : 블록스토리지 / 오브젝트 스토리지 / 공유 파일 시스템

- 볼륨 관리(about 블록 스토리지)
    - 인스턴스에 영구 스토리지를 제공하는 가장 일반적인 방법.
    - 블록 스토리지 서비스에서 관리.
    - 실제 시스템과 마찬가지로 원시 장치로 볼륨 제공(포맷 + 마운트 필요)
    - 블록 스토리지 서비스의 백엔드에 따라 다양한 유형 구현
    - 한번에 하나의 인스턴스에 연결(이동 가능)
    - 이미지 -> 볼륨 -> 인스턴스(영구적인 루트 스토리지 제공). 

- 오브젝트 컨테이너
    - 파일을 오브젝트로 저장할 수 있는 스토리지
    - 오브젝트를 컨테이너에 수집하고 특정한 액세스 권한을 구성.
    - API를 통한 접근
    - 클라우드 사용자가 데이터를 인스턴스에 업로드 하는데 적합.

- Manila 공유
    - 볼륨 상단에 분산 파일 시스템 구성(구 버전)
    - 동시에 여러 인스턴스에 마운트 가능.

- 블록 스토리지 설명
    - 볼륨을 스토리지 단위로 사용.
    - 볼륨을 인스턴스에 직접 연결하여 사용.
    - Ceph, NFS, SAN 등의 스토리지 백엔드 사용 가능
    - 사용 사례
        - 영구 또는 임시여야 하는 데이터를 저장하는 추가 공간.
        - 다른 인스턴스에서 분산 원시 장치를 기반으로 하는 분산 파일 시스템.
        - 분산 데이터베이스 등의 중요한 클라우드 기반 애플리케이션의 백엔드 스토리지.
    - 권장 사례
        - 프로덕션 환경에서 LVM은 권장하지 않음.
            - LVM : 논리 볼륨
        - LVM을 사용하여 컴퓨트 노드에서 인스턴스 가상 디스크를 관리.
        - 워크로드 요구 사항에 따라 적절한 스토리지 백엔드를 구성
        - 레거시 스토리지를 스토리지 계층으로 사용할 수 있도록 다중 백엔드를 구성
        - 스토리지 스케줄러를 구성하여 볼륨 요구 사항을 기반으로 백엔드에 볼륨 할당.

- 임시 스토리지용 백엔드 파일
    - 인스턴스를 만드는데 사용되는 플레이버에서 정의(디스크 사이즈)
    - 인스턴스 내부에 비영속성 스토리지 제공
    - 부팅 프로세스 동안 cloud-init 프로세스가 구성하는 인스턴스에 장치로 매핑.
        - cloud-init : 클라우드 프로비저닝시 초기 구성을 담당하는 프로세스.
    - 루트, 임시, 스왑 디스크
    - Ceph 스토리지 사용시 풀 종류 : images, volumes, vms

- 루트 디스크 관리
    - 인스턴스 배포시 이미지를 쓰기 가능한 방식으로 복사해서 사용
    - 이미지는 glance, 복사보는 libvirt에서 관리.

- 임시 디스크 관리
    - 원시 장치로 인스턴스에 매핑.
    - cloud-init으로 이 장치를 파일시스템 구성 및 /mnt 디렉터리에 마운트

- 스왑 디스크 관리
    - 원시 장치로 인스턴스에 매핑.
    - cloud-init으로 이 장치를 스왑으로 구성하고 인스턴스에서 스왑 메모리로 활성화.

- 임시 스토리지 사용 사례 검토
    - Cassandra는 궁극적으로 일관된 데이터베이스. NoSQL 종류의 데이터베이스이다. 분산 아키텍처에 적합.
    - 시작된 인스턴스는 Cassandra 클러스터에 들어간다.
    - 중앙 집중식 영구 스토리지가 필요 없음.
    - 컴퓨트 노드에서 고속 SSD 사용 가능.

### 영구 디스크 구성

- 레드햇 오픈스택 플랫폼의 영구 스토리지
    - 3가지 유형의 영구 스토리지 지원 : 블록, 오브젝트, 공유
    - 블록 스토리지는 볼륨 기반
    - 오브젝트 스토리지는 오브젝트 컨테이너 기반
    - 셀프 서비스 네트워크 스토리지는 공유파일시스템 서비스(Manila)에서 제공
    - overcloud 배포시 volumes 풀 생성
    - Cinder 서비스를 통해 관리.

- 영구 볼륨
    - 이미지 기반의 볼륨으로 인스턴스 생성 가능
    - 원한다면 별도의 볼륨 생성 후 인스턴스에 연결 가능
    - 원하는 파일 시스템으로 포맷
    - 분리 및 연결이 자유로움(데이터는 유지)
    - MultiAttach 유형으로 설정시 여러 인스턴스에 동시 연결이 가능하다.
    - 하나의 인스턴스에 여러개의 볼륨 연결 가능
    - iSCSI, NFS, Ceph 등 여러 블록 스토리지 유형에 맞는 드라이버 제공
    - 사용자가 직접 삭제하기 전까지 데이터 유지
    - 데이터베이스 및 레거시 서버 마이그레이션 등에 활용 가능

- 레거시 애플리케이션 스토리지 요구 사항 지원
    - 기존에는 아키텍처 변경 없이 마이그레이션 진행
        - 비용 증가 및 효율성 하락
    - 클라우드 환겨에 맞게 애플리케이션 재설계
        - 문제 방지 및 비용 절감
    - 다중 연결 사용
        - 클러스터형 시스템의 마이그레이션을 지원하기 위해 다중 연결 지원
        - 여러 인스턴스에서 단일 볼륨에 동시 접근 가능
        - Ceph에서만 가능하며 클러스터 인식이 되는 파일시스템만 가능

### 볼륨 및 스냅샷 관리

- 볼륨 관리
    - 클라우드 사용자는 볼륨 생성 및 인스턴스 연결 가능
    - Ceph사용시 다중 연결 기능을 지원한다.
    - 스냅샷 기능으로 볼륨 복제 및 복원 가능.
    - 볼륨 전송
        - 기본적으로는 하나의 프로젝트 내에서만 볼륨을 공유한다.
        - 볼륨을 사용하지 않을때에 다른 사람에게 볼륨을 전송할 수 있다.(공유가 아닌 양도)
        - 전송시 해당 사용자에게 ID & Key 전달.

- commands
    - `openstack server add volume <server name> <volume name>` : 서버에 볼륨 연결
    - `openstack server remove volume <server name> <volume name>` : 서버에서 볼륨 분리
    - `openstack volume delete <volume name>` : 볼륨 제거

    - `openstack volume transfer request create <volume name>` : 전송의 ID 및 인증키를 생성하는 본 명령어를 사용하여 요청을 생성한 후 제공해야 함. 볼륨 상태가 "in-use"상태에서는 불가하다. server remove 명령어로 available상태로 변환시킨 후 수행해야 한다.
    - `openstack volume transfer request accept --auth-key <auth key>` : 볼륨의 소유권 수락 명령어.

    - `openstack volume snapshot create --volume <volume name> <snapshot name>` : 볼륨 스냅샷 생성
        - `--force` : 인스턴스에 연결된 볼륨인 경우 추가해야하는 파라미터
    - `openstack volume backup create --volume <volume name>` : 볼륨 백업 생성.
    - `openstack volume snapshot list` : 스냅샷 리스트 확인.
    - `openstack volume snapshot delete` : 스냅샷 제거.
    


## Chapter 6. 추가 스토리지 전략 제공

### 오브젝트 스토리지 구현

- 오브젝트 스토리지 소게
    - Swift : 오브젝트 스토리지 관리 서비스 및 API(표준 / AWS). 오픈스택에서는 사용하나 레드햇 오픈스택에서는 셰프 사용.
    - 레드햇 오픈스택에서는 모든 종류의 기본 스토리지로 ceph 사용.
    - ceph api 사용시 오픈스택 환경 우회 -> swift서비스 사용.

- 오브젝트 스토리지의 활용 사례
    - 보관 및 백업
    - 빅데이터
    - 콘텐츠 레포지토리
    - 레코드 로깅
    - 데이터 레이크

- 오브젝트 특성 설명
    - 파일의 확장 특성에 저장되는 메타데이터와 함께 바이너리 파일로 저장. 각종 추가적인 기능(접근제어) 등이 설정될 수 있음.
    - 각 오브젝트는 GUID로 식별.
    - '/'기호로 오브젝트를 디렉토리 구조처럼 저장 가능.
    - 오브젝트는 컨테이너 단위로 목록화 및 접근제어

- 오브젝트 스토리지 기술
    - 스토리지 복제본은 시스템 중단 시 오브젝트 상태를 유지하는데 사용.
    - 최소 3개 이상의 복제본 권장
    - 복제본 저장 시 Zone 단위로 분산 보장

- 오브젝트 스토리지의 애플리케이션 사용 사례
    - 블록과 오브젝트의 가장 중요한 차이점은 인스턴스를 통하느냐 통하지않아도 되느냐의 차이가 있다.
    - swift에 저장된 모든 오브젝트에는 액세스 가능한 URL이 존재한다.
    - 모든 오픈스택 서비스 및 인스턴스에서 접근 가능한 완전 분산형 스토리지.
    - 소규모 오브젝트로 이루어진 대규모 풀에 사용하는 것에 효과적.
    - 대량의 데이터를 위한 가성비 좋은 스토리지(실사용량에 대한 비용 청구)
    - CDR 데이터는 비용청구, 모니터링, 사용패턴 분석 및 문제확인 등의 기능 제공
    - 여러명이 여러 사이트에서 동시 사용 가능.
    - 자체적인 분산/보호 기능 제공.

### 오브젝트 스토리지 기술 분석

- P228 오브젝트 스토리지 비교 표 참조.


### NFS 공유 스토리지 구현

- 기본 공유 파일 시스템 작업
    - 파일 공유 서비스 사용으로 애플리케이션 파일 공유 가능.
    - NFS or CIFS를 통해 파일을 공유. 네트워크를 이용해 파일을 공유하는 NAS방식의 파일공유시스템.
    - 애플리케이션과 파일 공유 백엔드 사이의 추상화 계층으로 파일 공유 서비스 필요.
    - 공유 파일 시스템에서 HA를 제공하려면 클라우드의 데이터 복제가 필요.
    - 어드민 역할의 사용자가 공유를 생성할 수 있도록 사용자 연결 api를 지원
    - api로 백엔드 구성, 공유 복제본 추가 및 제거, 스냅샷 및 액세스 규칙 관리 등 가능
    - 격리된 네트워크 사용시 보안에 유리
    - 생성은 관리자만 가능하지만 마운트는 누구나 할 수 있다.

- 공유파일 시스템 서비스(Manila) 설명
    - 마닐라 서비스 사용시 일반 사용자가 공유파일 시스템을 프로비저닝 및 관리 가능
    - 원격 파일 시스템으로 인스턴스의 공유 영역에 접근
    - 스토리지 리소스 자동 프로비저닝/관리 시 격리 및 확장 기능 제공
    - 다양한 프로토콜 지원 기능 제공.
    - 3가지 유형의 복제 지원
        - 기본 : 읽기/쓰기 가능 및 동기식으로 미러링
        - 보조 : 읽기 전용
        - DR : 재해 복구용으로 승격 전까지 권한 없음.

- 공유 파일 시스템 서비스 리소스
    - 공유 : 파일기반 스토리지(영구 스토리지)
    - 인스턴스에서 원격 파일 시스템으로 공유에 접근 가능(ro / rw)
    - 공유 유형 : 공유영역으로 사용할 백엔드 스토리지 지정(CephFS, GlusterFS 등)
    - 공유 유형을 분리해서 동일한 방식의 개별 설치 가능
    - 공유 네트워크 : 공유시 사용할 네트워크(각 프로젝트 네트워크에 바인딩)
    - 공유 접근 규칙은 특정 공유 단위로 사용자 별 접근제어

- NFS-Genesha
    - 인스턴스와 공유파일 시스템의 백엔드 스토리지 사이에 중재자 역할
    - 사용자의 접근제어 규칙 구성(클라이언트의 마운트 요청시 확인)
    - Kerberos 인증시 호환.

- Ceph 스토리지
    - Ceph 스토리지는 블록/오브젝트/파일 기반 스토리지 모두 지원
    - Ceph 스토리지는 데이터 및 메타데이터를 오브젝트로 저장
    - CephFS는 ceph클러스터에 대한 파일 기반 인터페이스
    - CephFS는 메타데이터 서버(MDS)를 실행하는 노드에서 관리 가능.



## Chapter 7. 공용 액세스가 가능한 인스턴스를 시작하도록 리소스 구성

### 프로바이더 및 외부 네트워크 관리

- 인스턴스 생성
    1. 이미지
    2. 플레이버
    3. 네트워크(프로젝트)
    4. 네트워크(외부)
    5. 3 + 4 => 라우터
    6. 유동IP
    7. ssh key / 보안 그룹

- 공용 애플리케이션의 OpenStack 네트워킹 설명
    - 공용 액세스용 인스턴스 배포방법 2가지
        - 프로바이더 네트워크 사용
            - 가장 일반적 유형
            - 관리자 권한이 있는 사용자만 생성 가능(사용은 모두)
            - 유형 : Local, flat, vlan, GRE, vxlan, GENEVE
            - 인스턴스 - 프로젝트 네트워크 - 라우터 - 외부 네트워크 연결 방식
        - 외부 네트워크로 연결이 라우팅되는 테넌트 네트워크 사용
            - 방화벽 규칙 및 애플리케이션의 네트워크 레이아웃 변경 가능

- 프로바이더 네트워크 사용 시 고려 사항
    - 프로젝트 네트워크 사용 시 서브넷 범위 및 DHCP 구성 자율
    - 프로바이더 네트워크는 기존 인프라 구성 내에서 작업
    - 네트워크 범위가 일치하지 않으면 라우팅 오류 발생
    - DHCP 중복시 IP주소 충돌 발생
    - 네트워크 유형은 물리적 네트워크 설계에 맞게 선택(flat, vlan)

### 라우터 및 유동IP 관리

- 테넌트 네트워크에서 공용 인스턴스 호스팅
    - 테넌트 네트워크와 라우터 연결
    - 라우터에 게이트웨이로 외부 네트워크 연결
    - IP주소를 생성할 할당 풀이 외부 네트워크에 존재
    - 유동 IP를 인스턴스에 추가.

- 라우터 소개
    - 서로 다른 네트워크 사이에 패킷을 전달하는 논리적 openstack 구성 요소
    - NAT 전달을 제공하여 외부 네트워크 접근
    - 레드햇 오픈스택에서는 SDN을 사용하여 라우팅 기능 제공.

- 유동 IP 주소 소개
    - 외부로 표시된 네트워크 풀에서 할당된 IP 주소
    - 유동 IP 주소 할당 풀은 외부네트워크와 동시에 생성
    - 공개적으로 연결할 수 있는 라우팅 가능 IP 주소
    - 외부 네트워크에서 인스턴스로 통신 가능
    - 인스턴스 시작 후 유동 IP 연결
    - 오픈스택 네트워킹 서비스에서 라우팅 규칙, 포트, Netfilter 규칙 등 자동 없데이트
    - 기존 인스턴스와 연결 해제시 다른 인스턴스에 연결 가능.

- 유동 IP 주소의 이점
    - 인스턴스 내에서 실행중인 서비스를 외부로 노출
    - 고급 네트워크 액세스 관리를 생성하기 위해 보안 그룹 관리
    - 복제 네트워크에 대한 접근 제한 규칙 설정 가능
    - 유동IP주소는 인스턴스와 함께 자동으로 연결 및 해제 가능
    - 오픈스택 네트워킹 서비스와 프로그래밍 방식으로 상호 작용하는 고가용성 솔루션

- 네트워크 패킷 처리
    - 라우터를 외부 네트워크와 연결하는 게이트웨이에서 NAT기능 제공
    - 라우터에 연결된 서브넷 중 하나의 개인ip와 유동ip의 정적 일대일 매핑 지원
    - L3라우팅 트래픽이 가상 라우터 통과 시 컴퓨트 노드로 직접 이동.
    
- commands
    - `openstack router create <router name>` : 라우터 생성
    - `openstack router set` : 라우터에 대한 설정값 지정
        - `--external-gateway` : 외부 네트워크 연결 파라미터
    - `openstack router add` : 라우터에 프로젝트 네트워크 연결.

### 보안 인스턴스 액세스 관리

- 키 페어 소개
    - SSH 키페어를 사용하면 암호없이 안전하게 신뢰할 수 있는 원격 서버에 접근 가능
    - 개인키/공개키 세트로 생성
    - 개인키 소유자만 인스턴스로 접속 가능
    - 새로 생성하거나 기존키를 가져올 수 있다.
    - 공개키는 컴퓨트 서비스의 데이터베이스에 저장
    - 하나의 키로 여러 인스턴스 사용 가능
    - 하나의 사용자가 여러 키를 사용 가능
    - 개인키 분실시 복구 안됨.
    - 권한은 600으로 설정되어 있어야 사용가능하다. 644,660등이면 안된다. 키 생성시 바로 600으로 바꿔줘야 할 것.

- commands
    - `openstack keypair create --private-key <keyname.pem> <keyname>` : 키 생성. 반드시 `--private-key`옵션을 사용하여 저장하여 주어야 사용할 수 있다. 저장시 확장자는 pem.
    - `openstack keypair create --public-key <public key path> <keyname>` : 퍼블릭키가 이미 있다면  사용

- 시큐리티 그룹 소개
    - 인스턴스에 대한 또는 인스턴스로부터의 네트워크 패킷 액세스를 제어
    - 프로젝트 구성원은 보안 그룹에 대한 규칙 편집 및 규칙 추가 가능
    - 모든 프로젝트에 디폴트 보안 그룹 제공
    - 기본적으로 보안 그룹에서 나가는 패킷은 all allow, 들어오는 패킷은 all deny.

- ovn 구현
    - 기존 OVS에서는 iptables를 사용하며 Linux 브리지 및 복잡한 추가 계층 사용.
    - OVN에서는 conntrack을 통해서 Statefull ACL 제공
    - stateful방식으로 추가 계층 제거 및 분산 패킷 필터링 방식 제공.

### 공용 액세스가 가능한 인스턴스 시작 및 확인

- 인스턴스 생성시 ssh 키 추가(`--key-name`옵션 사용). 생성시에만 지정가능
- 인스턴스 생성시 보안 그룹 추가(`--security-group`옵션 사용). 생성 후에도 지정 가능.
- 프로바이더 네트워크에 직접 연결 가능
- 프로젝트 네트워크에 연결 시 라우터를 통해 외부네트워크에 연결
- 프로젝트 네트워크에 연결시 유동ip설정 필요.

- 외부 -> 내부 인커밍이 안된다면 보안 그룹 문제. 포트번호 확인 필요.
- 다 되어 있는데 서비스가 동작이 안된다면? 인스턴스 접속하여 systemctl명령어로 서비스 확인을 해보아야 한다.